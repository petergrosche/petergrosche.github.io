[{"authors":["admin"],"categories":null,"content":"I am a creative engineer with a background in digital signal processing and data science and a passion for audio and music. With a strong hands-on and can-do attitude, I enjoy working on new and challenging audio technologies and love to deliver results which create a great user experience.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://petergrosche.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a creative engineer with a background in digital signal processing and data science and a passion for audio and music. With a strong hands-on and can-do attitude, I enjoy working on new and challenging audio technologies and love to deliver results which create a great user experience.","tags":null,"title":"Peter Grosche","type":"authors"},{"authors":null,"categories":null,"content":"    Experience          Audio Technologies Researcher at Huawei Technologies, Munich Research Center   01.2017 \u0026ndash; present  Principal Research Engineer     Audio technology specialist for media playback, speech enhancement, AR/VR and Audio Intelligence applications for smartphones\nDesigning DSP solutions for loudspeaker correction and protection, microphone array speech processing, and VR 3D audio rendering\nDeveloping AI systems for media content and music genre recognition (DNN), acoustic scene and environment recognition (SVM) and source separation (LSTM)\nEnsuring deliverables from R\u0026amp;D of next generation audio solutions to products\nImplementation of algorithms in Matlab, Pyton, float C utilizing TensorFlow, Keras, CURRENNT, and TF Lite toolkits   01.2015 \u0026ndash; 12.2016  Senior Research Engineer     Developing DSP solutions for sound playback enhancement (micro-speaker and headphones) and voice communication (uplink and downlink)\nExtending SWS: incorporating algorithms for stereo to 5 channel upmixing, dialogue enhancement, and crosstalk cancellation\nResearching speech enhancement algorithms for voice communication, microphone array noise reduction and speech intelligibility enhancement\nImplementing algorithms in Matlab and float C; developing real-time demonstrators using VST and Android SDK/NDK toolkits\nDelivering algorithms to tablets (Mediapads M2, M3) and smartphones (Mate 9)\nManaging technical collaboration projects with universities and SMEs Securing IPR, filing 30+ PCT applications   11.2012 \u0026ndash; 12.2014  Research Engineer 3D Audio     Designing a complete DSP solution for sound playback enhancement with increased sound quality on smartphones\nDelivering the Huawei SWS (Super Wide Sound) sound enhancement solution to Huawei Mediapad products (replacing the omnipresent DTS suite)\nDeveloping algorithms: EQ, DRC, limiting, bass enhancement (missing fundamental), binaural rendering and crosstalk cancellation\nRapid prototyping, product related research and demonstration of audio DSP algorithms to stakeholders and product lines\nImplementing a complete sound enhancement solution satisfying product requirements (Matlab), writing efficient code meeting hardware limitations (floating point C) and assisting fixed-point integration (Tensilica HiFi)   Music Information Retrieval Researcher at Saarland University, Saarbr\u0026uuml;cken   06.2008 \u0026ndash; 10.2012  Research and teaching in the field of audio and music signal processing and music information retrieval     Developing audio DSP algorithms for music analysis, feature extraction, and efficient content-based search in large music databases     Researching beat tracking and tempo estimation algorithms which can cope with time variying tempo and classical music without drums; repetition based music segmentation and structure analysis (chorus detection); music retrieval and cover song recognition based on Locality-sensitive hashing (LSH) for efficient similarity search; music transcription and chord recognition   Research intern at NYU Music and Audio Research Laboratory, New York   10.2010 \u0026ndash; 11.2010  Developed an unsupervised musical motif discovery system using sparse convolutive Non-Negative Matrix Factorization (NMF) in Python   Research intern at the Audio \u0026amp; Multimedia Department at Fraunhofer IIS, Erlangen   02.2007 \u0026ndash; 05.2007  Surround Upmix: Development of algorithms for converting stereo to 5.1 (Matlab)     Instrument recognition in polyphonic music signals based on Hidden Markov Models (HMM) using HTK Speech Recognition toolkit   Research assistant at the Institute for Human-Machine Communication, TUM   12.2005 \u0026ndash; 01.2006  Assisted in building a multimodal database for developing and evaluating\nemotion recognition procedures   Intern at Akustikb\u0026uuml;ro Schwartzenberger \u0026amp; Burkhart, P\u0026ouml;cking, Germany.   02.2005 \u0026ndash; 04.2005  Room acoustic measurements, simulations and optimizations        Education          PhD in Computer Science at Saarland University and Max-Planck-Institut Informatik, Saarbr\u0026uuml;cken, Germany   06.2008 \u0026ndash; 11.2012  Title of PhD thesis: \u0026quot;Signal Processing Methods for Beat Tracking, Music Segmentation, and Audio Retrieval\u0026quot;     20+ publications at scientific conferences, journals and books     Supervised by Prof. Meinard M\u0026uuml;ller     Degree: Dr.-Ing, Grade: magna cum laude (very good)   Master in Electrical Engineering and Information Technology at TU M\u0026uuml;nchen, Germany   10.2004 \u0026ndash; 05.2008  Main focus on Digital Signal Processing, Pattern Recognition, Information Retrieval and Human-Machine Interaction; Personal focus on audio and music analysis     Diploma thesis: \u0026quot;Polyphonic Music Transcription based on Real Audio\u0026quot;\nSupervised by Prof. Bj\u0026ouml;rn Schuller\nDeveloped a complete system deriving a symbolic MIDI representation from a polyphonic audio recording; combining mulit-pitch estimation, onset detection and Hidden Markov Models (HMM) for pitch tracking.      Degree: Dipl.-Ing. Univ., Grade: 1.5 (very good)   Studies abroad: Lunds Universitet, Lund, Sweden   08.2006 \u0026ndash; 01.2007  Selected courses in Machine Learning and statistics   Undergraduate studies in Electrical Engineering and Information Technology at TU M\u0026uuml;nchen, Germany   10.2002 \u0026ndash; 10.2004  Main focus on Information Technology and Communication Engineering     Bachelor thesis: \u0026quot;An Application for Automatically Organizing, Structuring, and Analyzing Large Music Collections\u0026quot;     Developed a foobar2000 component adding genre recognition and music similarity search functionality based on custom feature extraction and SVM classification; Tools: Weka, LIBSVM, C++     Degree: B.Sc., Grade: 1.6 (good)   ","date":1561680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561680000,"objectID":"fd36605688ef45e10dc233c860158012","permalink":"https://petergrosche.github.io/cv/","publishdate":"2019-06-28T00:00:00Z","relpermalink":"/cv/","section":"","summary":"Peter Grosche - CV","tags":null,"title":"CV","type":"page"},{"authors":null,"categories":null,"content":" Information in accordance with Section 5 TMG Peter Grosche\nLimmatstr. 3\n81476 München\nContact Information Telephone: 01788818922\nE-Mail: peter.grosche@gmail.com\nDisclaimer The contents on my homepage have been created with the adequate care. However, I cannot guarantee that the contents are accurate, complete or up-to-date. According to statutory provisions, I am responsible for my own content but I am not obliged to monitor the information of third parties or investigate signs of illegal activity. My obligation to prevent the use of information under generally applicable laws remains unaffected by this as per §§ 8 to 10 of the Telemedia Act (TMG). Responsibility for the content of linked external pages remains solely with the operators of the linked pages. No violations were evident to me at the time of linking. Should any legal infringement become known to me, I will remove the link immediately.\nCopyright My contents are subject to German copyright law. Unless expressly permitted by law, every form of utilizing, reproducing or processing content on my web pages requires the prior consent of the respective owner of the rights. Any unauthorized use may violate copyright laws.\n","date":1561680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561680000,"objectID":"0f2867d7019ab25f2097792dc222c701","permalink":"https://petergrosche.github.io/legal/","publishdate":"2019-06-28T00:00:00Z","relpermalink":"/legal/","section":"","summary":"Peter Grosche - Legal Disclosure","tags":null,"title":"Legal Disclosure","type":"page"},{"authors":["Fabian Brinkmann","Manoj Dinakaran","Robert Pelzer","Peter Grosche","Daniel Voss","Stefan Weinzierl"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"137bfe2a1f0e72fe2d0ad3133780c978","permalink":"https://petergrosche.github.io/publication/brinkmann-2019-cross/","publishdate":"2021-08-16T20:37:04.60755Z","relpermalink":"/publication/brinkmann-2019-cross/","section":"publication","summary":"The individualization of head related transfer functions (HRTFs) can make an important contribution to improving the quality of binaural technology applications. One approach to individualization is to exploit the relationship between the shape of HRTFs and the anthropometric features of the ears, head, and torso of the corresponding listeners. To identify statistically significant relationships between the two sets of variables, a relatively large database is required. For this purpose full-spherical HRTFs of 96 subjects were acoustically measured and numerically simulated. A detailed cross-evaluation showed a good agreement to previous data between repeated measurements and between measured and simulated data. In addition to 96 HRTFs, the database includes high-resolution head-meshes, a list of 25 anthropometric features per subject, and headphone transfer functions for two headphone models.","tags":null,"title":"A cross-evaluated database of measured and simulated HRTFs including 3D head meshes, anthropometric features, and headphone impulse responses","type":"publication"},{"authors":["Brinkmann AND Manoj, Dinakaran AND Robert, Pelzer AND Jan Joschka, Wohlgemuth AND Fabian, Seipel AND Daniel, Voss AND Peter, Grosche AND Stefan, Weinzierl Fabian"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"ac39bc46195778e6aea5ddeb9df57691","permalink":"https://petergrosche.github.io/publication/11303-9429/","publishdate":"2021-08-16T20:37:04.610547Z","relpermalink":"/publication/11303-9429/","section":"publication","summary":"The database contains head-related transfer functions, 3D head meshes, anthropometric features, and headphone transfer functions of 96 subjects.","tags":["HRTF"],"title":"The HUTUBS head-related transfer function (HRTF) database","type":"publication"},{"authors":["Juan Pablo Bello","Peter Grosche","Meinard Müller","Ron Weiss"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"c6f0af841004835319d8e77fed3a2f36","permalink":"https://petergrosche.github.io/publication/bello-2018-content/","publishdate":"2019-09-16T17:45:00.453178Z","relpermalink":"/publication/bello-2018-content/","section":"publication","summary":"This chapter presents several computational approaches aimed at supporting knowledge discovery in music. Our work combines data mining, signal processing and data visualization techniques for the automatic analysis of digital music collections, with a focus on retrieving and understanding musical structure. We discuss the extraction of midlevel feature representations that convey musically meaningful information from audio signals, and show how such representations can be used to synchronize different instances of a musical work and enable new modes of music content browsing and navigation. Moreover, we utilize these representations to identify repetitive structures and representative patterns in the signal, via self-similarity analysis and matrix decomposition techniques that can be made invariant to changes of local tempo and key. We discuss how structural information can serve to highlight relationships within music collections, and explore the use of information visualization tools to characterize the patterns of similarity and dissimilarity that underpin such relationships. With the help of illustrative examples computed on a collection of recordings of Frédéric Chopin’s Mazurkas, we aim to show how these content-based methods can facilitate the development of novel modes of access, analysis and interaction with digital content that can empower the study and appreciation of music.","tags":null,"title":"Content-based Methods for Knowledge Discovery in Music","type":"publication"},{"authors":["Manoj Dinakaran","Fabian Brinkmann","Stine Harder","Robert Pelzer","Peter Grosche","Rasmus R Paulsen","Stefan Weinzierl"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"6d985c09e05a7eb799144d58ca5b46ff","permalink":"https://petergrosche.github.io/publication/dinakaran-2018-perceptually/","publishdate":"2019-09-16T17:45:00.468165Z","relpermalink":"/publication/dinakaran-2018-perceptually/","section":"publication","summary":"Numerical simulations offer a feasible alternative to the direct acoustic measurement of individual head-related transfer functions (HRTFs). For the acquisition of high quality 3D surface scans, as required for these simulations, several approaches exist. In this paper, we systematically analyze the variations between different approaches and evaluate the influence of the accuracy of 3D scans on the resulting simulated HRTFs. To assess this effect, HRTFs were numerically simulated based on 3D scans of the head and pinna of the FABIAN dummy head generated with 6 different methods. These HRTFs were analyzed in terms of interaural time difference , interaural level difference, energetic error in auditory filters and by their modeled localization performance. From the results, it is found that a geometric precision of about 1 mm is needed to maintain accurate localization cues, while a precision of about 4 mm is sufficient to maintain the overall spectral shape.","tags":null,"title":"Perceptually motivated analysis of numerically simulated head-related transfer functions generated by various 3D surface scanning systems","type":"publication"},{"authors":["Mikko Parviainen","Pasi Pertilä","Tuomas Virtanen","Peter Grosche"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"87eebf39d7ae43f66b5ab5fd73ab0393","permalink":"https://petergrosche.github.io/publication/parviainen-2018-time/","publishdate":"2019-09-16T17:45:00.502134Z","relpermalink":"/publication/parviainen-2018-time/","section":"publication","summary":"This paper presents a low-latency neural network based speech enhancement system. Low-latency operation is critical for speech communication applications. The system uses the time-frequency (TF) masking approach to retain speech and remove the non-speech content from the observed signal. The ideal TF mask are obtained by supervised training of neural networks. As the main contribution different neural network models are experimentally compared to investigate computational complexity and speech enhancement performance. The proposed system is trained and tested on noisy speech data where signal-to-noise ratio (SNR) ranges from -5 dB to +5 dB and the results show significant reduction of non-speech content in the resulting signal while still meeting a low-latency operation criterion, which is here considered to be less than 20 ms.","tags":null,"title":"Time-Frequency Masking Strategies for Single-Channel Low-Latency Speech Enhancement Using Neural Networks","type":"publication"},{"authors":["Manoj Dinakaran","Peter Grosche","Fabian Brinkmann","Stefan Weinzierl"],"categories":null,"content":"","date":1462060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462060800,"objectID":"a167e4b469a213e515bdb6ea4d437200","permalink":"https://petergrosche.github.io/publication/dinakaran-2016-extraction/","publishdate":"2019-09-16T17:45:00.437192Z","relpermalink":"/publication/dinakaran-2016-extraction/","section":"publication","summary":"Anthropometric measures are used for individualizing head-related transfer functions (HRTFs) for example, by selecting best match HRTFs from a large library or by manipulating HRTF with respect to anthropometrics. Within this process, an accurate extraction of anthropometric measures is crucial as small changes may already influence the individualization. Anthropometrics can be measured in many different ways, e.g., from pictures or anthropometers. However, these approaches tend to be inaccurate. Therefore, we propose to use Kinect for generating individual 3D head-and-shoulder meshes from which anthropometrics are automatically extracted. This is achieved by identifying and measuring distances between characteristics points on the outline of each mesh and was found to yield accurate and reliable estimates of corresponding features. In our experiment, a large set of anthropometric measures was automatically extracted for 61 subjects and evaluated in terms of a cross-validation against the manually extracted correspondent.","tags":null,"title":"Extraction of anthropometric measures from 3d-meshes for the individualization of head-related transfer functions","type":"publication"},{"authors":["Christian Schörkhuber","Matthias Frank","Franz Zotter","Robert Höldrich","Peter Grosche"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"f726e02ea9b6a281449acb6a780eb9e3","permalink":"https://petergrosche.github.io/publication/schorkhuber-2016-automatic/","publishdate":"2019-09-16T17:45:00.523112Z","relpermalink":"/publication/schorkhuber-2016-automatic/","section":"publication","summary":"The aim of immersive teleconferencing is to convey a realistic sound field impression to a remote participant. To this end, the spatial distribution of talkers as well as room information needs to be captured by the near-end system and accurately reproduced on the far-end. We consider a setup where high speech quality is obtained by means of several close microphones and spatial information is captured with a small microphone array in the centre of the acoustic scene. The proposed automatic mixing system robustly estimates the directions of multiple active talkers and mixes the close-microphone signals with the room information gathered by the central microphone array. Furthermore, we propose a novel automatic gain control method that keeps natural speech dynamics while equalizing speech level fluctuations due to unintentional changes of the talker-microphone distance. To allow for maximal flexibility concerning the reproduction system on the far-end (e.g. different loudspeaker setups or binaural reproduction for headphones), the sound field is encoded in higher-order Ambisonics. Listening experiments of our concluding evaluation indicate the optimal settings for recorded multi-talker scenarios using both headphone- and loudspeaker-based reproduction.","tags":null,"title":"Automatic Mixing for Immersive Teleconferencing Systems","type":"publication"},{"authors":["Jürgen T Geiger","Peter Grosche","Yesenia Lacouture Parodi"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"d8da9a2a477e842a279ce1c5d42c84aa","permalink":"https://petergrosche.github.io/publication/geiger-2015-dialogue/","publishdate":"2019-09-16T17:45:00.423205Z","relpermalink":"/publication/geiger-2015-dialogue/","section":"publication","summary":"Studies show that many people have difficulties in understanding dialogue in movies when watching TV, especially hard-of-hearing listeners or in adverse listening environments. In order to overcome this problem, we propose an efficient methodology to enhance the speech component of a stereo signal. The method is designed with low computational complexity in mind, and consists of first extracting a center channel from the stereo signal. Novel methods for speech enhancement and voice activity detection are proposed which exploit the stereo information. A speech enhancement filter is estimated based on the relationship between the extracted center channel and all other channels. Subjective and objective evaluations show that this method can successfully enhance intelligibility of the dialogue without affecting the overall sound quality negatively.","tags":null,"title":"DIALOGUE ENHANCEMENT OF STEREO SOUND","type":"publication"},{"authors":["Giovanni Cordara","Nicola Piotto","Sergio Garcı́a Lobo","Francisco Morán Burgos","Davide Bertola","Leonardo Chiariglione","Peter Grosche","Marius Preda","Milos Markovic","Alberto Messina"," others"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"886cf9d277abf1ecab2713c6e4795bfc","permalink":"https://petergrosche.github.io/publication/cordara-2014-d-7/","publishdate":"2019-09-16T17:45:00.481153Z","relpermalink":"/publication/cordara-2014-d-7/","section":"publication","summary":"Connected and Social media BRIDGET is opening new dimensions for multimedia content creation and consumption by enhancing broadcast programmes with bridgets.  A bridget links from the programme you are watching to second screen, interactive media such as web pages, images, audio clips, different types of video (2D, multi-view, with depth information, free viewpoint) and synthetic 3D models. Bridgets can be: • created automatically or manually by broadcasters, either from their own content (e.g., archives, Internet and other services) or from wider Internet sources; • created by end users, either from their local archives or from Internet content; • transmitted in the broadcast stream or independently; • filtered by a recommendation engine based on user profile, relevance, quality, etc.; • enjoyed on the common main screen or a private second screen, in a user-centric and immersive manner, e.g., within 3D models allowing users to place themselves inside an Augmented Reality (AR) scene at the exact location from which the linked content was captured. To deliver the above, BRIDGET developed: • a hybrid broadcast/Internet architecture; • a professional Authoring Tool (AT) to generate bridgets and dynamic AR scenes with spatialised audio; • an easy-to-use AT for end users; • a player to select bridgets, and consume and navigate the resulting dynamic AR scenes. The AT and player will use a range of sophisticated and innovative technologies extending state-of-the-art 3D scene reconstruction, media analysis and visual search; enabling customised and context-adapted hybrid broadcast/Internet services offering enhanced interactive, multi-screen, social and immersive content for new forms of AR experiences. BRIDGET tools will be based on and contribute to international standards, thus ensuring the creation of a true horizontal market and ecosystem for connected TV and contributed media applications..","tags":null,"title":"D7. 1: BRIDGET Authoring Tools and Player","type":"publication"},{"authors":["Christian Kirst","Felix Weninger","Cyril Joder","Peter Grosche","Jürgen Geiger","Björn Schuller"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"6fe41e7ce62ff617ce1b0d03a5a82cf8","permalink":"https://petergrosche.github.io/publication/kirst-2014-line/","publishdate":"2019-09-16T17:45:00.38524Z","relpermalink":"/publication/kirst-2014-line/","section":"publication","summary":"Speech de-noising algorithms often suffer from introduction of artifacts, either by removal of parts of the speech signal, or imperfect noise reduction causing the remaining noise to sound unnatural and disturbing. This contribution proposes to spatially distribute monaural noisy speech signals based on single-channel source separation, in order to improve the perceived speech quality. Stereo up-mixing is utilized on the estimated speech and noise sources instead of simply suppressing the noise. This paper investigates the case of non-negative matrix factorization (NMF) speech enhancement applied to high levels of non-stationary noise. NMF-based and spectral subtraction speech enhancement algorithms are evaluated in a listening test in terms of speech intelligibility, presence of interfering noises and overall quality with respect to the unprocessed signal. In the result, the listening test provides evidence for superior noise reduction by NMF, yet also a drop in perceived speech quality that is not covered by the employed set of common objective metrics. However, stereo up-mixing of NMF-separated speech and noise delivers high subjective noise reduction while preserving the perceived speech quality.","tags":null,"title":"On-line NMF-based Stereo Up-Mixing of Speech Improves Perceived Reduction of Non-Stationary Noise","type":"publication"},{"authors":["Joan Serrà","Meinard Müller","Peter Grosche","Josep Ll Arcos"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"b0d5384374e87b7096371d30267a563c","permalink":"https://petergrosche.github.io/publication/serra-2014-unsupervised/","publishdate":"2019-09-16T17:45:00.403224Z","relpermalink":"/publication/serra-2014-unsupervised/","section":"publication","summary":"Automatically inferring the structural properties of raw multimedia documents is essential in today’s digitized society. Given its hierarchical and multi-faceted organization, musical pieces represent a challenge for current computational systems. In this article, we present a novel approach to music structure annotation based on the combination of structure features with time series similarity. Structure features encapsulate both local and global properties of a time series, and allow us to detect boundaries between homogeneous, novel, or repeated segments. Time series similarity is used to identify equivalent segments, corresponding to musically meaningful parts. Extensive tests with a total of five benchmark music collections and seven different human annotations show that the proposed approach is robust to different ground truth choices and parameter settings. Moreover, we see that it outperforms previous approaches evaluated under the same framework.","tags":null,"title":"Unsupervised Music Structure Annotation by Time Series Structure Features and Segment Similarity","type":"publication"},{"authors":["Peter Matthias Grosche"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"747a3429da3b3e0f509c53ad29636a1a","permalink":"https://petergrosche.github.io/publication/grosche-2013-signal/","publishdate":"2019-09-16T17:45:00.293327Z","relpermalink":"/publication/grosche-2013-signal/","section":"publication","summary":"The goal of music information retrieval (MIR) is to develop novel strategies and techniques for organizing, exploring, accessing, and understanding music data in an efficient manner. The conversion of waveform-based audio data into semantically meaningful feature representations by the use of digital signal processing techniques is at the center of MIR and constitutes a difficult field of research because of the complexity and diversity of music signals. In this thesis, we introduce novel signal processing methods that allow for extracting musically meaningful information from audio signals. As main strategy, we exploit musical knowledge about the signals' properties to derive feature representations that show a significant degree of robustness against musical variations but still exhibit a high musical expressiveness. We apply this general strategy to three different areas of MIR: Firstly, we introduce novel techniques for extracting tempo and beat information, where we particularly consider challenging music with changing tempo and soft note onsets. Secondly, we present novel algorithms for the automated segmentation and analysis of folk song field recordings, where one has to cope with significant fluctuations in intonation and tempo as well as recording artifacts. Thirdly, we explore a cross-version approach to content-based music retrieval based on the query-by-example paradigm. In all three areas, we focus on application scenarios where strong musical variations make the extraction of musically meaningful information a challenging task.","tags":null,"title":"Signal processing methods for beat tracking, music segmentation, and audio retrieval","type":"publication"},{"authors":["Peter Grosche","Meinard Müller","Joan Serrà"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"32edc8d4477130ee162353951bbc7cf1","permalink":"https://petergrosche.github.io/publication/grosche-2013-towards/","publishdate":"2019-09-16T17:45:00.367258Z","relpermalink":"/publication/grosche-2013-towards/","section":"publication","summary":"In this paper we investigate whether we can extract the commonalities shared by a group of cover songs or versions of the same musical piece. As a main contribution, we introduce the concept of cover group thumbnail, which is the most representative, essential subsequence for an entire group of versions. Opposed to previous approaches, we jointly consider all versions of a given song to compute a single cover group template, which then shows a high degree of robustness against version-specific aspects. To compute such a template, we introduce a modification of a recent audio thumbnailing technique. To evaluate the reliability of our conceptual contribution, we consider the task of template-based version identification, where we show comparable accuracies to existing systems.","tags":null,"title":"Towards cover group thumbnailing","type":"publication"},{"authors":["Meinard Müller","Nanzhu Jiang","Peter Grosche"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"d605733b684acc39ae0b48d7b351c642","permalink":"https://petergrosche.github.io/publication/muller-2012-robust/","publishdate":"2019-09-16T17:45:00.273345Z","relpermalink":"/publication/muller-2012-robust/","section":"publication","summary":"The automatic extraction of structural information from music recordings constitutes a central research topic. In this paper, we deal with a subproblem of audio structure analysis called audio thumbnailing with the goal to determine the audio segment that best represents a given music recording. Typically, such a segment has many (approximate) repetitions covering large parts of the recording. As the main technical contribution, we introduce a novel fitness measure that assigns a fitness value to each segment that expresses how much and how well the segment “explains” the repetitive structure of the entire recording. The thumbnail is then defined to be the fitness-maximizing segment. To compute the fitness measure, we describe an optimization scheme that jointly performs two error-prone steps, path extraction and grouping, which are usually performed successively. As a result, our approach is even able to cope with strong musical and acoustic variations that may occur within and across related segments. As a further contribution, we introduce the concept of fitness scape plots that reveal global structural properties of an entire recording. Finally, to show the robustness and practicability of our thumbnailing approach, we present various experiments based on different audio collections that comprise popular music, classical music, and folk song field recordings.","tags":null,"title":"A robust fitness measure for capturing repetitions in music recordings with applications to audio thumbnailing","type":"publication"},{"authors":["Peter Grosche","Meinard Müller","Joan Serrà"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"73fc515155231ec3c627e8ffeb1fac9c","permalink":"https://petergrosche.github.io/publication/grosche-ms-12-content-based-music-retrieval-dagstuhl-fu/","publishdate":"2019-09-16T17:45:00.181431Z","relpermalink":"/publication/grosche-ms-12-content-based-music-retrieval-dagstuhl-fu/","section":"publication","summary":"The rapidly growing corpus of digital audio material requires novel retrieval strategies for ex-ploring large music collections. Traditional retrieval strategies rely on metadata that describe the actual audio content in words. In the case that such textual descriptions are not available, one requires content-based retrieval strategies which only utilize the raw audio material. In this contribution, we discuss content-based retrieval strategies that follow the query-by-example paradigm: given an audio query, the task is to retrieve all documents that are somehow similar or related to the query from a music collection. Such strategies can be loosely classified according to their specificity, which refers to the degree of similarity between the query and the database documents. Here, high specificity refers to a strict notion of similarity, whereas low specificity to a rather vague one. Furthermore, we introduce a second classification principle based on gran-ularity, where one distinguishes between fragment-level and document-level retrieval. Using a classification scheme based on specificity and granularity, we identify various classes of retrieval scenarios, which comprise audio identification, audio matching, and version identification. For these three important classes, we give an overview of representative state-of-the-art approaches, which also illustrate the sometimes subtle but crucial differences between the retrieval scenarios. Finally, we give an outlook on a user-oriented retrieval system, which combines the various re-trieval strategies in a unified framework.","tags":null,"title":"Audio Content-Based Music Retrieval","type":"publication"},{"authors":["Meinard Müller","Peter Grosche"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"3b787239b84099517f410b91b0230381","permalink":"https://petergrosche.github.io/publication/muller-2012-automated/","publishdate":"2019-09-16T17:45:00.25836Z","relpermalink":"/publication/muller-2012-automated/","section":"publication","summary":"In this paper, we introduce an automated procedure for segmenting a given folk song field recording into its constituent stanzas. One challenge arises from the fact that these recordings are performed by elderly non-professional singers under poor recording conditions such that the constituent stanzas may reveal significant temporal and spectral deviations. Unlike a previously described segmentation approach that relies on a manually transcribed reference stanza, we introduce a reference-free segmentation procedure, which is driven by an audio thumbnailing procedure in combination with enhanced similarity matrices. Our experiments on a Dutch folk song collection show that our segmentation results are comparable to the ones obtained by the reference-based method.","tags":null,"title":"Automated segmentation of folk song field recordings","type":"publication"},{"authors":["Peter Grosche","Björn Schuller","Meinard Müller","Gerhard Rigoll"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"024d27fd982c8957d524c3e595f53ba8","permalink":"https://petergrosche.github.io/publication/grosche-2012-automatic/","publishdate":"2019-09-16T17:45:00.165447Z","relpermalink":"/publication/grosche-2012-automatic/","section":"publication","summary":"The automatic transcription of music recordings with the objective to derive a score-like representation from a given audio representation is a fundamental and challenging task. In particular for polyphonic music recordings with overlapping sound sources, current transcription systems still have problems to accurately extract the parameters of individual notes specified by pitch, onset, and duration. In this article, we present a music transcription system that is carefully designed to cope with various facets of music. One main idea of our approach is to consistently employ a mid-level representation that is based on a musically meaningful pitch scale. To achieve the necessary spectral and temporal resolution, we use a multi-resolution Fourier transform enhanced by an instantaneous frequency estimation. Subsequently, having extracted pitch and note onset information from this representation, we employ Hidden Markov Models (HMM) for determining the note events in a context-sensitive fashion. As another contribution, we evaluate our transcription system on an extensive dataset containing audio recordings of various genre. Here, opposed to many previous approaches, we do not only rely on synthetic audio material, but evaluate our system on real audio recordings using MIDI-audio synchronization techniques to automatically generate reference annotations.","tags":null,"title":"Automatic transcription of recorded music","type":"publication"},{"authors":["Peter Grosche","Joan Serrà","Meinard Müller","Josep Lluı́s Arcos"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"9daea29403c47a8094f27cd64914e61c","permalink":"https://petergrosche.github.io/publication/grosche-2012-structure/","publishdate":"2019-09-16T17:45:00.352272Z","relpermalink":"/publication/grosche-2012-structure/","section":"publication","summary":"Content-based approaches to music retrieval are of great relevance as they do not require any kind of manually generated annotations. In this paper, we introduce the concept of structure fingerprints, which are compact descriptors of the musical structure of an audio recording. Given a recorded music performance, structure fingerprints facilitate the retrieval of other performances sharing the same underlying structure. Avoiding any explicit determination of musical structure, our fingerprints can be thought of as a probability density function derived from a self-similarity matrix. We show that the proposed fingerprints can be compared by using simple Euclidean distances without using any kind of complex warping operations required in previous approaches. Experiments on a collection of Chopin Mazurkas reveal that structure fingerprints facilitate robust and efficient content-based music retrieval. Furthermore, we give a musically informed discussion that also deepens the understanding of this popular Mazurka dataset.","tags":null,"title":"Structure-Based Audio Fingerprinting for Music Retrieval","type":"publication"},{"authors":["Peter Grosche","Meinard Müller"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"e949abf6c86a1e2897ce9ab1aebe4e70","permalink":"https://petergrosche.github.io/publication/grosche-2012-toward/","publishdate":"2019-09-16T17:45:00.202412Z","relpermalink":"/publication/grosche-2012-toward/","section":"publication","summary":"The general goal of cross-version music retrieval is to identify all versions of a given piece of music by means of a short query audio fragment. To speed up the retrieval process, hashing techniques have been proposed, where the audio material is split up into small overlapping shingles (used as hashes) that consist of short feature subsequences. In this paper, we extend this work with the goal to minimize the number of hash lookups. To this end, one requires larger shingles that characterize the underlying piece of music to a high degree, while being robust to variations that occur across different versions. As our main contribution, we report on extensive experiments to highlight the delicate trade-off between the query length, feature parameters, shingle dimension, and index settings. These insights are of fundamental importance for building efficient cross-version retrieval systems that scale to millions of songs.","tags":null,"title":"Toward characteristic audio shingles for efficient cross-version music retrieval","type":"publication"},{"authors":["Peter Grosche","Meinard Müller"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"33136c62ede86c35db8ab7956a157448","permalink":"https://petergrosche.github.io/publication/grosche-2012-toward-1/","publishdate":"2019-09-16T17:45:00.221394Z","relpermalink":"/publication/grosche-2012-toward-1/","section":"publication","summary":"In this paper, we investigate to which extent well-known audio fingerprinting techniques, which aim at identifying a specific audio recording, can be modified to also deal with more musical variations. To this end, we replace the standard peak fingerprints based on a spectrogram by peak fingerprints based on other more “musical” feature representations. Our systematic experiments show that such modified peak fingerprints allow for a robust identification of different versions and performances of the same piece of music if the query length is at least 15 seconds. This indicates that highly efficient audio fingerprinting techniques can also be applied to accelerate tasks such as audio matching or cover song identification.","tags":null,"title":"Toward Musically-Motivated Audio Fingerprints","type":"publication"},{"authors":["Joan Serrà","Meinard Müller","Peter Grosche","Josep Lluis Arcos"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"5f39336994457f057dd7708faea7e40c","permalink":"https://petergrosche.github.io/publication/serra-mga-12-boundary-detection-aaai/","publishdate":"2019-09-16T17:45:00.241375Z","relpermalink":"/publication/serra-mga-12-boundary-detection-aaai/","section":"publication","summary":"Locating boundaries between coherent and/or repetitive segments of a time series is a challenging problem pervading many scientific domains. In this paper we propose an unsupervised method for boundary detection, combining three basic principles: novelty, homogeneity, and repetition. In particular, the method uses what we call structure features, a representation encapsulating both local and global properties of a time series. We demonstrate the usefulness of our approach in detecting music structure boundaries, a task that has received much attention in recent years and for which exist several benchmark datasets and publicly available annotations. We find our method to significantly outperform the best accuracies published so far. Importantly, our boundary approach is generic, thus being applicable to a wide range of time series beyond the music and audio domains.","tags":null,"title":"Unsupervised detection of music boundaries by time series structure features","type":"publication"},{"authors":["Hendrik Schreiber","Peter Grosche","Meinard Müller"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"389715c33bf00099ed37f0c656f553a3","permalink":"https://petergrosche.github.io/publication/schreiber-gm-11-finger-print-ismir/","publishdate":"2019-09-16T17:45:00.003597Z","relpermalink":"/publication/schreiber-gm-11-finger-print-ismir/","section":"publication","summary":"The Haitsma/Kalker audio fingerprinting system has been in use for years, but its search algorithm’s scalability has not been researched very well. In this paper we show that by simple re-ordering of the query fingerprint’s sub-prints in the index-based retrieval step, the overall search performance can be increased significantly. Furthermore, we show that combining longer fingerprints with re-ordering can lead to even higher performance gains, up to a factor of 9.8. The proposed re-ordering scheme is based on the observation that sub-prints, which are elements of n-runs of identical consecutive sub-prints, have a higher survival rate in distorted copies of a signal (e.g. after mp3 compression) than other sub-prints.","tags":null,"title":"A Re-ordering Strategy for Accelerating Index-based Audio Fingerprinting","type":"publication"},{"authors":["Meinard Müller","Peter Grosche","Nanzhu Jiang"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"71a171905f8a183fb91aaf4404a5903e","permalink":"https://petergrosche.github.io/publication/muller-2011-segment/","publishdate":"2019-09-16T17:45:00.025576Z","relpermalink":"/publication/muller-2011-segment/","section":"publication","summary":"In this paper, we deal with the task of determining the audio segment that best represents a given music recording (similar to audio thumbnailing). Typically, such a segment has many (approximate) repetitions covering large parts of the music recording. As main contribution, we introduce a novel fitness measure that assigns to each segment a fitness value that expresses how much and how well the segment \"explains\" the repetitive structure of the recording. In combination with enhanced feature representations, we show that our fitness measure can cope even with strong variations in tempo, instrumentation, and modulations that may occur within and across related segments. We demonstrate the practicability of our approach by means of several challenging examples including field recordings of folk music and recordings of classical music.","tags":null,"title":"A Segment-based Fitness Measure for Capturing Repetitive Structures of Music Recordings","type":"publication"},{"authors":["Nanzhu Jiang","Peter Grosche","Verena Konz","Meinard Müller"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"75fe52e147f4bc31ae1f98e5d88af9d9","permalink":"https://petergrosche.github.io/publication/jiang-2011-analyzing/","publishdate":"2019-09-16T17:45:00.149461Z","relpermalink":"/publication/jiang-2011-analyzing/","section":"publication","summary":"The computer-based harmonic analysis of music recordings with the goal to automatically extract chord labels directly from the given audio data constitutes a major task in music information retrieval. In most automated chord recognition procedures, the given music recording is first converted into a sequence of chroma-based audio features and then pattern matching techniques are applied to map the chroma features to chord labels. In this paper, we analyze the role of the feature extraction step within the recognition pipeline of various chord recognition procedures based on template matching strategies and hidden Markov models. In particular, we report on numerous experiments which show how the various procedures depend on the type of the underlying chroma feature as well as on parameters that control temporal and spectral aspects.","tags":null,"title":"Analyzing Chroma Feature Types for Automated Chord Recognition","type":"publication"},{"authors":["Peter Grosche","Meinard Müller"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"21c6551122ff3c144530e97d7349ad62","permalink":"https://petergrosche.github.io/publication/grosche-2010-extracting/","publishdate":"2019-09-16T17:44:59.951644Z","relpermalink":"/publication/grosche-2010-extracting/","section":"publication","summary":"The extraction of tempo and beat information from music recordings constitutes a challenging task in particular for non-percussive music with soft note onsets and time-varying tempo. In this paper, we introduce a novel mid-level representation that captures musically meaningful local pulse information even for the case of complex music. Our main idea is to derive for each time position a sinusoidal kernel that best explains the local periodic nature of a previously extracted note onset representation. Then we employ an overlap-add technique accumulating all these kernels over time to obtain a single function that reveals the predominant local pulse (PLP). Our concept introduces a high degree of robustness to noise and distortions resulting from weak and blurry onsets. Furthermore, the resulting PLP curve reveals the local pulse information even in the presence of continuous tempo changes and indicates a kind of confidence in the periodicity estimation. As further contribution, we show how our PLP concept can be used as a flexible tool for enhancing tempo estimation and beat tracking. The practical relevance of our approach is demonstrated by extensive experiments based on music recordings of various genres.","tags":null,"title":"Extracting predominant local pulse information from music recordings","type":"publication"},{"authors":["Peter Grosche","Meinard Müller"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"a4075ca39a32726ade8cc041d487517f","permalink":"https://petergrosche.github.io/publication/grosche-2011-tempogram/","publishdate":"2019-09-16T17:45:00.129479Z","relpermalink":"/publication/grosche-2011-tempogram/","section":"publication","summary":"The extraction of local tempo and pulse information from audio recordings constitutes a challenging task, in particular for music with significant tempo variations. Furthermore, the existence of various pulse levels such as measure, tactus,  and  tatum  often  makes  the  determination  of  absolute tempo problematic. In this demo, we present the tempogram toolbox, which contains MATLAB implementations for extracting various types of recently proposed tempo and pulse-related audio representations. These representations are particularly designed to reveal useful information even for music  with  weak  note  onset  information  and  changing tempo. The  toolbox is provided  under a GNU-GPL license.","tags":null,"title":"Tempogram toolbox: Matlab implementations for tempo and pulse analysis of music recordings","type":"publication"},{"authors":["Meinard Müller","Verena Konz","Peter Grosche","Nanzhu Jiang","Zhe Zuo"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"bb11c43876dc01e5fc7056cc953786eb","permalink":"https://petergrosche.github.io/publication/muller-2010-novel/","publishdate":"2019-09-16T17:45:00.056547Z","relpermalink":"/publication/muller-2010-novel/","section":"publication","summary":"In this demo we present novel functionalities for a user interface referred to as Interpretation Switcher, which has emerged from the previously developed SyncPlayer sys-tem [1]. This interface allows a user to select several recordings of the same piece of music, which have pre-viously been synchronized [2]. ","tags":null,"title":"A Novel Timeline Adjustment Functionality for the Interpretation Switcher","type":"publication"},{"authors":["Meinard Müller","Peter Grosche","Frans Wiering"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"44313c4328bdc081715eec1bca033dfb","permalink":"https://petergrosche.github.io/publication/muller-2010-automated/","publishdate":"2019-09-16T17:44:59.983615Z","relpermalink":"/publication/muller-2010-automated/","section":"publication","summary":"Performance analysis of recorded music material has become increasingly important in musicological research and music psychology. In this paper, we present various techniques for extracting performance aspects from field recordings of folk songs. Main challenges arise from the fact that the recorded songs are performed by non-professional singers, who deviate significantly from the expected pitches and timings even within a single recording of a song. Based on a multimodal approach, we exploit the existence of a symbolic transcription of an idealized stanza in order to analyze a given audio recording of the song that comprises a large number of stanzas. As the main contribution of this paper, we introduce the concept of chroma templates by which consistent and inconsistent aspects across the various stanzas of a recorded song are captured in the form of an explicit and semantically interpretable matrix representation. Altogether, our framework allows for capturing differences in various musical dimension such as tempo, key, tuning, and melody.","tags":null,"title":"Automated analysis of performance variations in folk song recordings","type":"publication"},{"authors":["Peter Grosche","Meinard Müller","Frank Kurth"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"3d20e0257c0597495d78c90351261fff","permalink":"https://petergrosche.github.io/publication/grosche-2010-cyclic/","publishdate":"2019-09-16T17:44:59.924669Z","relpermalink":"/publication/grosche-2010-cyclic/","section":"publication","summary":"The extraction of local tempo and beat information from audio recordings constitutes a challenging task, particularly for music that reveals significant tempo variations. Furthermore, the existence of various pulse levels such as measure, tactus, and tatum often makes the determination of absolute tempo problematic. In this paper, we present a robust mid-level representation that encodes local tempo information. Similar to the well-known concept of cyclic chroma features, where pitches differing by octaves are identified, we introduce the concept of cyclic tempograms, where tempi differing by a power of two are identified. Furthermore, we describe how to derive cyclic tempograms from music signals using two different methods for periodicity analysis and finally sketch some applications to tempo-based audio segmentation.","tags":null,"title":"Cyclic Tempogram -- A Mid-level Tempo Representation For Music Signals","type":"publication"},{"authors":["Peter Grosche","Meinard Müller","Frank Kurth"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"a2f628e3ee793ed4d375093d1d8c3373","permalink":"https://petergrosche.github.io/publication/grosche-2010-tempobasierte/","publishdate":"2019-09-16T17:45:00.04256Z","relpermalink":"/publication/grosche-2010-tempobasierte/","section":"publication","summary":"Die automatische Segmentierung und Strukturierung ist für die automatisierte Verarbeitung von Musikaufnahmen von grundlegender Bedeutung. Das Ziel der Musiksegmentierung ist die Zerlegung eines Musikstücks in semantisch sinnvolle Abschnitte und elementare Einheiten. Diese Zerlegung eines Audiodatenstroms kann dann die Grundlage bilden fur eine anschließende Weiterverarbeitung der Musikdaten, wie z.B. eine Klassifikation, Annotation oder Indexierung. In diesem Beitrag stellen wir Merkmale vor die das lokal vorherrschende Tempo eines Musikstücks beschreiben und die Anforderungen hinsichtlich Robustheit und Dimensionalität erfüllen. Des Weiteren diskutieren wir wie mit diesen Merkmalen eine tempobasierte Segmentierung vorgenommen werden kann indem das Musikstück in Abschnitte mit homogenem Tempo eingeteilt wird und zeigen wie dies eine sinnvolle Ergänzung zur Segmentierung hinsichtlich der Klangfarbe oder Harmonie eines Musikstücks darstellt.","tags":null,"title":"Tempobasierte Segmentierung von Audioaufnahmen","type":"publication"},{"authors":["Peter Grosche","Meinard Müller","Craig Stuart Sapp"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"bfe6969ae08ca5c72fa16338c8c3a42e","permalink":"https://petergrosche.github.io/publication/grosche-2010-makes/","publishdate":"2019-09-16T17:44:59.937657Z","relpermalink":"/publication/grosche-2010-makes/","section":"publication","summary":"The automated extraction of tempo and beat information from music recordings is a challenging task. Especially in the case of expressive performances, current beat tracking approaches still have significant problems to accurately capture local tempo deviations and beat positions. In this paper, we introduce a novel evaluation framework for detecting critical passages in a piece of music that are prone to tracking errors. Our idea is to look for consistencies in the beat tracking results over multiple performances of the same underlying piece. As another contribution, we further classify the critical passages by specifying musical properties of certain beats that frequently evoke tracking errors. Finally, considering three conceptually different beat tracking procedures, we conduct a case study on the basis of a challenging test set that consists of a variety of piano performances of Chopin Mazurkas. Our experimental results not only make the limitations of state-of-the-art beat trackers explicit but also deepens the understanding of the underlying music material.","tags":null,"title":"What makes beat tracking difficult? A case study on Chopin Mazurkas","type":"publication"},{"authors":["Peter Grosche","Meinard Müller"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"fbbef4b586adc9edd035321454c1003b","permalink":"https://petergrosche.github.io/publication/grosche-m-09-tempogram-ismir/","publishdate":"2019-09-16T17:44:59.897694Z","relpermalink":"/publication/grosche-m-09-tempogram-ismir/","section":"publication","summary":"Automated beat tracking and tempo estimation from music recordings become challenging tasks in the case of non-percussive music with soft note onsets and time-varying tempo. In this paper, we introduce a novel mid-level representation which captures predominant local pulse information. To this end, we first derive a tempogram by performing a local spectral analysis on a previously extracted, possibly very noisy onset representation. From this, we derive for each time position the predominant tempo as well as a sinusoidal kernel that best explains the local periodic nature of the onset representation. Then, our main idea is to accumulate the local kernels over time yielding a single function that reveals the predominant local pulse (PLP). We show that this function constitutes a robust mid-level representation from which one can derive musically meaningful tempo and beat information for non-percussive music even in the presence of significant tempo fluctuations. Furthermore, our representation allows for incorporating prior knowledge on the expected tempo range to exhibit information on different pulse levels.","tags":null,"title":"A mid-level representation for capturing dominant tempo and pulse information in music recordings","type":"publication"},{"authors":["Peter Grosche","Meinard Müller","Sebastian Ewert"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"8513ecf9b708b93ab59860aae1f5c6da","permalink":"https://petergrosche.github.io/publication/grosche-2009-combination/","publishdate":"2019-09-16T17:45:00.102504Z","relpermalink":"/publication/grosche-2009-combination/","section":"publication","summary":"Many different methods for the detection of note onsets in music recordings have been proposed and applied to tasks such as music transcription, beat tracking, tempo estimation, and music synchronization.  Most of the proposed onset detectors rely on the fact that note onsets often go along with a sudden increase of the signal’s energy, which particularly holds for instruments such as piano, guitar, or percussive instruments.  Much more difficult is the detection of onsets in the case of more fluent note transitions, which is often the case for classical music dominated by string instruments. In this paper, we introduce improved novelty curves that yield good indications for note onsets even in the case of only smooth temporal and spectral intensity changes in the signal.We then show how these novelty curves can be used to significantly improve the temporal accuracy in music synchronization tasks. ","tags":null,"title":"Combination of Onset-Features with Applications to High-Resolution Music Synchronization","type":"publication"},{"authors":["Peter Grosche","Meinard Müller"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"9f0fb17d907d4150b4f611db0da45f1d","permalink":"https://petergrosche.github.io/publication/grosche-2009-computing/","publishdate":"2019-09-16T17:44:59.910682Z","relpermalink":"/publication/grosche-2009-computing/","section":"publication","summary":"The periodic structure of musical events plays a crucial role in the perception of tempo as well as the sensation of note changes and onsets. In this paper, we introduce a novel function that reveals the predominant local periodicity (PLP) in music recordings. Here, our main idea is to estimate for each time position a periodicity kernel that best explains the local periodic nature of previously extracted note onset information and then to accumulate all these kernels to yield a single function. This function, which is also referred to as PLP curve, reveals musically meaningful periodicity information even for non-percussive music with soft and blurred note onsets. Such information is useful not only for stabilizing note onset detection but also for beat tracking and tempo estimation in the case of music with changing tempo.","tags":null,"title":"Computing predominant local periodicity information in music recordings","type":"publication"},{"authors":["Sebastian Ewert","Meinard Müller","Peter Grosche"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"ffcc74463ebfe9960d5c6cc456ca99d4","permalink":"https://petergrosche.github.io/publication/ewert-2009-high/","publishdate":"2019-09-16T17:44:59.884706Z","relpermalink":"/publication/ewert-2009-high/","section":"publication","summary":"The general goal of music synchronization is to automatically align the multiple information sources such as audio recordings, MIDI files, or digitized sheet music related to a given musical work. In computing such alignments, one typically has to face a delicate tradeoff between robustness and accuracy. In this paper, we introduce novel audio features that combine the high temporal accuracy of onset features with the robustness of chroma features. We show how previous synchronization methods can be extended to make use of these new features. We report on experiments based on polyphonic Western music demonstrating the improvements of our proposed synchronization framework.","tags":null,"title":"High resolution audio synchronization using chroma onset features","type":"publication"},{"authors":["Meinard Müller","Peter Grosche","Frans Wiering"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"65edc6fb245299570cbfe889ef739aa4","permalink":"https://petergrosche.github.io/publication/mueller-gw-09-folk-song-annotation-ismir/","publishdate":"2019-09-16T17:44:59.968628Z","relpermalink":"/publication/mueller-gw-09-folk-song-annotation-ismir/","section":"publication","summary":"Even though folk songs have been passed down mainly by oral tradition, most musicologists study the relation between folk songs on the basis of score-based transcriptions. Due to the complexity of audio recordings, once having the transcriptions, the original recorded tunes are often no longer studied in the actual folk song research though they still may contain valuable information. In this paper, we introduce an automated approach for segmenting folk song recordings into its constituent stanzas, which can then be made accessible to folk song researchers by means of suitable visualization, searching, and navigation interfaces. Performed by elderly non-professional singers, the main challenge with the recordings is that most singers have serious problems with the intonation, fluctuating with their voices even over several semitones throughout a song. Using a combination of robust audio features along with various cleaning and audio matching strategies, our approach yields accurate segmentations even in the presence of strong deviations.","tags":null,"title":"Robust Segmentation and Annotation of Folk Song Recordings","type":"publication"},{"authors":["Meinard Müller","Peter Grosche","Frans Wiering"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"817ed3adbd05dc5eed37e6b86bb8b362","permalink":"https://petergrosche.github.io/publication/muller-2009-towards/","publishdate":"2019-09-16T17:45:00.07753Z","relpermalink":"/publication/muller-2009-towards/","section":"publication","summary":"Folk music is closely related to the musical culture of a specific nation or region. Even though folk songs have been passed down mainly by oral tradition, most musicologists study the relation between folk songs on the basis of symbolic music descriptions, which are obtained by transcribing recorded tunes into a score-like representation. Due to the complexity of audio recordings, once having the transcriptions, the original recorded tunes are often no longer used in the actual folk song research even though they still may contain valuable information. In this paper, we present various techniques for making audio recordings more easily accessible for music researchers. In particular, we show how one can use synchronization techniques to automatically segment and annotate the recorded songs. The processed audio recordings can then be made accessible along with a symbolic transcript by means of suitable visualization, searching, and navigation interfaces to assist folk song researchers to conduct large scale investigations comprising the audio material. ","tags":null,"title":"Towards automated processing of folk song recordings","type":"publication"},{"authors":["Peter Matthias Grosche"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"2d6fbfc44309b4a0065439bb67ad6726","permalink":"https://petergrosche.github.io/publication/grosche-2008-poly/","publishdate":"2019-09-16T17:45:00.313308Z","relpermalink":"/publication/grosche-2008-poly/","section":"publication","summary":"Automatic music transcription is a challenging task that has been the topic of many research groups since 1977. The problem has been addressed in several manners and suitable results were obtained for the monophonic case, where a single note is played at a time. Still, the transcription of real-world polyphonic music with arbitrary instruments, genre and tempi suffers from many unsolved problems. The two key sub-problems in automatic music transcription necessary to obtain a note-level parameter representation of a musical piece are polyphonic pitch estimation and notes’ onset detection. Both tasks are addressed in this thesis for real-world music. Effective signal representations are proposed that adapt the properties of music signals. The spectral characteristics are described with a semitone spectrogram that receives enhanced time/frequency resolution through the use of a multiresolution DFT and instantaneous frequency estimation. Nonharmonic signal components are suppressed using RASTA processing. The temporal characteristics are modeled in complex domain, taking envelope and frequency continuity conditions into account. An iterative approach is proposed for the polyphonic pitch estimation that detects the predominant F0 in a signal and uses harmonic spectrum estimation to extract it, before the estimation is continued with the residual. The onset and duration detection of each note is resolved through temporal properties of the signal. In addition, Hidden Markov Models are used to model each note as a distinct event with both, spectral and temporal characteristics simultaneously to determine its pitch, onset and offset. To further enhance the results achievable with the methods based on signal properties, musical knowledge is used, namely the musical key to find probable notes in a song, and musical meter to define probable on- and offset positions. The evaluation of the proposed methods is carried out on a database of acoustic recordings, whose reference transcription are obtained through force alignment with midi files. Note precision and recall rates above 60% are achieved, together with a temporal accuracy of 40%.","tags":null,"title":"Polyphonic Music Transcription based on Real Audio","type":"publication"},{"authors":["Peter Matthias Grosche"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"68e7cfdf9e876376cb15da3d034364db","permalink":"https://petergrosche.github.io/publication/grosche-2006-ba/","publishdate":"2019-09-16T17:45:00.335288Z","relpermalink":"/publication/grosche-2006-ba/","section":"publication","summary":"Diese Arbeit beschäftigt sich mit der Organisation und Strukturierung von Musikdatenbanken. Der Schwerpunkt der entwickelten Methoden liegt auf der inhaltsbasierten Analyse von Musikstücken. Hierzu wurden Merkmale untersucht, die anhand der spektralen und periodischen Eigenschaften die Empfindungen Klangfarbe und Rhythmus modellieren. Die inhaltsbasierte Analyse anhand dieser Merkmale beinhaltet einerseits die Klassifikation der Musik nach verschiedenen Gesichtspunkten, wie Genre Erkennung und Erkennen von Live - Aufnahmen, als auch den Vergleich der Ähnlichkeit verschiedener Lieder über eine Abstandsberechnung. Im Rahmen der Arbeit wurden Anwendungen zur Vereinfachung der Organisation von Musikdatenbanken entwickelt und in den Foobar2000 Audio Player integriert. Unter anderem sind dies eine Genre Erkennung mit Trainingsmöglichkeit durch den Benutzer, eine inhaltsbasierte Suche nach ähnlichen Musikstücken und eine textbasierte Suche mit Fehlertoleranz auf Grundlage der Levenstein Distance.","tags":null,"title":"Entwicklung einer Applikation zur automatischen Strukturierung und Analyse grosser Musikdatenbanken","type":"publication"}]