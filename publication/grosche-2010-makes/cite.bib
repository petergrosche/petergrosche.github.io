@inproceedings{grosche2010makes,
 abstract = {The automated extraction of tempo and beat information from music recordings is a challenging task. Especially in the case of expressive performances, current beat tracking approaches still have significant problems to accurately capture local tempo deviations and beat positions. In this paper, we introduce a novel evaluation framework for detecting critical passages in a piece of music that are prone to tracking errors. Our idea is to look for consistencies in the beat tracking results over multiple performances of the same underlying piece. As another contribution, we further classify the critical passages by specifying musical properties of certain beats that frequently evoke tracking errors. Finally, considering three conceptually different beat tracking procedures, we conduct a case study on the basis of a challenging test set that consists of a variety of piano performances of Chopin Mazurkas. Our experimental results not only make the limitations of state-of-the-art beat trackers explicit but also deepens the understanding of the underlying music material.},
 address = {Utrecht, The Netherlands},
 author = {Peter Grosche and Meinard MÃ¼ller and Craig Stuart Sapp},
 booktitle = {Proceedings of the 11th International Conference on Music Information Retrieval (ISMIR)},
 doi = {10.5281/zenodo.1415852},
 pages = {649--654},
 title = {What makes beat tracking difficult? A case study on Chopin Mazurkas},
 url = {https://www.researchgate.net/publication/220723198},
 year = {2010}
}

